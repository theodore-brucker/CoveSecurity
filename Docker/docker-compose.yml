services:

  # Service classifications
  # Class: Critical - Services that are critical to the operation of the system
  # Class: High - Services that are important to the operation of the system, but do not cause other services to fail
  # Class: Low - Services that are a nice to have, but do not cause other services to fail or effect primary features

  # Necessary for Kafka coordination
  # Class: Critical
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    networks:
      - kafka_network

  # Handles high speed data transfer between services
  # Class: Critical
  kafka:
    image: confluentinc/cp-kafka:latest
    ports:
      - "9092:9092"
      - "29092:29092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONSUMER_MAX_POLL_INTERVAL_MS: 900000
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_LOG_RETENTION_HOURS: 1
      KAFKA_DELETE_TOPIC_ENABLE: 'true'
      KAFKA_OPTS: "-Djava.net.preferIPv4Stack=true"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - zookeeper
    networks:
      kafka_network:
        aliases:
          - kafka

  # Used as a helper to ensure that the kafka topics are created
  # before any services attempt to use them.
  # Class: Critical
  init-kafka:
    image: confluentinc/cp-kafka:latest
    depends_on:
      - kafka

    # This container runs the command, and after it finishes it turns itself off
    entrypoint: [ '/bin/sh', '-c' ]
    command: |
      "
      echo -e 'Creating kafka topics'
      # blocks until kafka is reachable
      kafka-topics --bootstrap-server kafka:9092 --list

      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic raw_data --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic processed_data --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic predictions --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic training_data --replication-factor 1 --partitions 1
      kafka-topics --bootstrap-server kafka:9092 --create --if-not-exists --topic labeled_data --replication-factor 1 --partitions 1

      echo -e 'Successfully created the following topics:'
      kafka-topics --bootstrap-server kafka:9092 --list
      "
    networks:
      - kafka_network
     
  # Primary component for serving and training models
  # Class: High
  torchserve:
    image: ghcr.io/theodore-brucker/covesecurity/torchserve:latest
    ports:
      - "8080:8080"
      - "8081:8081"
      - "8082:8082"
    environment:
      - EARLY_STOPPING_THRESHOLD=.001
      - NUM_EPOCHS=25
      - SEQUENCE_LENGTH=16
      - FEATURE_COUNT=12
      - MODEL_STORE_PATH=/home/model-server/model-store/
    volumes:
      - ./model_store:/home/model-server/model-store
    networks:
      - kafka_network

  # Primary interface for user interacting with the system
  # Class: Critical
  web_backend:
    image: ghcr.io/theodore-brucker/covesecurity/web_backend:latest
    volumes:
      - ./web/backend:/app
      - training_data:/app/training_data
    environment:
      - training_data:/app/training_data
      # Environment variables for Kakfa
      - KAFKA_BROKER=kafka:9092
      - RAW_TOPIC=raw_data
      - PROCESSED_TOPIC=processed_data
      - PREDICTION_TOPIC=predictions
      - TRAINING_TOPIC=training_data
      - LABELED_TOPIC=labeled_data

      # Networking variables
      - MAX_RETRIES=5
      - RETRY_DELAY=2
      - DATA_PROCESSING_URL=http://172.17.0.1:5001
      - MONGO_URI=mongodb://mongodb:27017/
    ports:
      - "5000:5000"
    networks:
      kafka_network:
        aliases:
          - web_backend
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - kafka
      - mongodb

  # Frontend for the web interface
  # Class: High
  web_frontend:
    image: ghcr.io/theodore-brucker/covesecurity/web_frontend:latest
    environment:
      - REACT_APP_BACKEND_URL=http://web_backend:5000
    volumes:
      - ./web/frontend:/app
    ports:
      - "3000:3000"
    networks:
      - kafka_network

  # Collects data from the network interface, processes it, and pushes it to Kafka
  # Class: High
  data_processing:
    image: ghcr.io/theodore-brucker/covesecurity/data_processing:latest
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    volumes:
      - training_data:/app/training_data
    environment:

    # Environment variables for Kakfa
      - KAFKA_BROKER=localhost:29092
      - RAW_TOPIC=raw_data
      - PROCESSED_TOPIC=processed_data
      - PREDICTIONS_TOPIC=predictions
      - TRAINING_TOPIC=training_data
      - CAPTURE_INTERFACE=eth0

    # Environment variables for TorchServe
      - TORCHSERVE_REQUESTS=http://localhost:8080
      - TORCHSERVE_MANAGEMENT=http://localhost:8081
      - TORCHSERVE_METRICS=http://localhost:8082
      - ANOMALY_THRESHOLD=.5
      - training_data:/app/training_data
      - SEQUENCE_LENGTH=16
      - FEATURE_COUNT=12
      - MODEL_NAME=transformer_autoencoder

    # Networking variables
      - WEB_BACKEND_URL=http://localhost:5000
      - MONGO_URI=mongodb://mongodb:27017/
      - FLASK_PORT=5001

    depends_on:
      - kafka
      - torchserve
      - web_backend
      - mongodb
    extra_hosts:
      - "host.docker.internal:host-gateway"

  # Stores data for the system
  # Class: Low
  mongodb:
    image: mongo:latest
    ports:
      - "27017:27017"
    volumes:
      - ./database/init-mongo.js:/docker-entrypoint-initdb.d/init-mongo.js:ro
      
      # Comment out the next line and line 266 if you want to start with a fresh database on every restart
      - mongodb_data:/data/db
    networks:
      - kafka_network

  # Web interface for the MongoDB
  # Class: Low
  mongo-express:
    image: mongo-express
    ports:
      - "28081:8081"
    environment:
      ME_CONFIG_MONGODB_URL: mongodb://mongodb:27017/admin
      ME_CONFIG_BASICAUTH_USERNAME: root
      ME_CONFIG_BASICAUTH_PASSWORD: example
    networks:
      - kafka_network
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - mongodb
    restart: always

  # Connects Kafka topics to MongoDB
  # Class: High
  kafka-connect:
    image: confluentinc/cp-kafka-connect:latest
    depends_on:
      - kafka
    volumes:
      - ./kafka/create-topics.sh:/create-topics.sh:ro
      - ./kafka/connect-mongodb-sink.json:/connect-mongodb-sink.json:ro
    command:
      - bash
      - -c
      - |
        /create-topics.sh
        echo "Waiting for topics to be created..."
        sleep 10
        confluent-hub install --no-prompt mongodb/kafka-connect-mongodb:latest
        /etc/confluent/docker/run &
        echo "Waiting for Kafka Connect to start..."
        sleep 60
        echo "Listing installed plugins:"
        ls -R /usr/share/java /usr/share/confluent-hub-components
        echo "Attempting to create connector:"
        curl -v -X POST -H "Content-Type: application/json" --data @/connect-mongodb-sink.json http://localhost:8083/connectors
        echo "Connector creation attempt complete. Checking connector status:"
        curl -v http://localhost:8083/connectors
        echo "Entering sleep loop..."
        while true; do sleep 30; done
    environment:
      CONNECT_BOOTSTRAP_SERVERS: kafka:9092
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: "compose-connect-group"
      CONNECT_CONFIG_STORAGE_TOPIC: "docker-connect-configs"
      CONNECT_OFFSET_STORAGE_TOPIC: "docker-connect-offsets"
      CONNECT_STATUS_STORAGE_TOPIC: "docker-connect-status"
      CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.storage.StringConverter"
      CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_LOG4J_ROOT_LOGLEVEL: "INFO"
      CONNECT_LOG4J_LOGGERS: "org.apache.kafka.connect.runtime.rest=WARN,org.reflections=ERROR"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_PLUGIN_DISCOVERY: "only_scan"
      CONNECT_REPLICATION_FACTOR: 1
    ports:
      - "8083:8083"
    networks:
      - kafka_network

networks:
  kafka_network:
    driver: bridge

volumes:
  training_data:
  # Comment out the next line and line 187 if you want to start with a fresh database on every restart
  mongodb_data: