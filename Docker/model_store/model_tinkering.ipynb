{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from scapy.all import rdpcap, IP, TCP, UDP, PcapReader\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PacketSequenceDataset(Dataset):\n",
    "    def __init__(self, pcap_file, sequence_length, feature_dim, max_packets=50000, label=None):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_dim = feature_dim\n",
    "        self.label = label\n",
    "        self.scalers = [StandardScaler() for _ in range(feature_dim)]\n",
    "        self.max_packets = max_packets\n",
    "        self.data = self._parse_pcap(pcap_file)\n",
    "        self._compute_feature_averages(self.data)\n",
    "        self.data = self._scale_data(self.data)\n",
    "\n",
    "    def _parse_pcap(self, pcap_file):\n",
    "        try:\n",
    "            with PcapReader(pcap_file) as pcap_reader:\n",
    "                packets = [pcap_reader.read_packet() for _ in range(self.max_packets)]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read pcap file: {e}\")\n",
    "            return np.zeros((1, self.sequence_length, self.feature_dim), dtype=np.float32)\n",
    "        data = []\n",
    "\n",
    "        def safe_convert(value):\n",
    "            if isinstance(value, (int, float)):\n",
    "                return value\n",
    "            try:\n",
    "                return int(value)\n",
    "            except:\n",
    "                try:\n",
    "                    return float(value)\n",
    "                except:\n",
    "                    return 0\n",
    "\n",
    "        expected_feature_count = self.feature_dim  # Use the initial feature_dim\n",
    "\n",
    "        for packet in packets:\n",
    "            features = []\n",
    "            # IP features\n",
    "            try:\n",
    "                if IP in packet:\n",
    "                    ip = packet[IP]\n",
    "                    features.extend([\n",
    "                        int.from_bytes(bytes(map(int, ip.src.split('.'))), byteorder='big'),\n",
    "                        int.from_bytes(bytes(map(int, ip.dst.split('.'))), byteorder='big'),\n",
    "                        safe_convert(ip.len),\n",
    "                        safe_convert(ip.flags),\n",
    "                        #safe_convert(ip.frag),\n",
    "                        safe_convert(ip.ttl),\n",
    "                        safe_convert(ip.proto)\n",
    "                    ])\n",
    "                else:\n",
    "                    features.extend([0] * 7)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error parsing IP features: {e}\")\n",
    "                features.extend([0] * 7)\n",
    "\n",
    "            # TCP/UDP features\n",
    "            try:\n",
    "                if TCP in packet:\n",
    "                    tcp = packet[TCP]\n",
    "                    features.extend([\n",
    "                        safe_convert(tcp.sport),\n",
    "                        safe_convert(tcp.dport),\n",
    "                        safe_convert(tcp.flags),\n",
    "                        safe_convert(tcp.window)\n",
    "                    ])\n",
    "                elif UDP in packet:\n",
    "                    udp = packet[UDP]\n",
    "                    features.extend([\n",
    "                        safe_convert(udp.sport),\n",
    "                        safe_convert(udp.dport),\n",
    "                        safe_convert(udp.len)\n",
    "                    ])\n",
    "                    features.append(0)  # Padding to match TCP feature count\n",
    "                else:\n",
    "                    features.extend([0] * 4)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error parsing TCP/UDP features: {e}\")\n",
    "                features.extend([0] * 4)\n",
    "\n",
    "            # General packet features\n",
    "            try:\n",
    "                features.extend([\n",
    "                    safe_convert(len(packet))\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error parsing general packet features: {e}\")\n",
    "                features.extend([0, 0.0])\n",
    "\n",
    "            if len(features) < expected_feature_count:\n",
    "                features.extend([0] * (expected_feature_count - len(features)))\n",
    "            elif len(features) > expected_feature_count:\n",
    "                features = features[:expected_feature_count]    \n",
    "\n",
    "            data.append(features)\n",
    "\n",
    "        num_samples = max(1, len(data) // self.sequence_length)\n",
    "        padded_data = np.zeros((num_samples, self.sequence_length, self.feature_dim), dtype=np.float32)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            start_idx = i * self.sequence_length\n",
    "            end_idx = min(start_idx + self.sequence_length, len(data))\n",
    "            padded_data[i, :end_idx - start_idx, :] = data[start_idx:end_idx]\n",
    "\n",
    "        logger.info(f\"Parsed {len(data)} packets into {num_samples} samples with {self.feature_dim} features each\")\n",
    "        return np.array(padded_data, dtype=np.float32)\n",
    "    \n",
    "    def _compute_feature_averages(self, data):\n",
    "        feature_means = np.mean(data, axis=(0, 1))\n",
    "        feature_stds = np.std(data, axis=(0, 1))\n",
    "        for i, (mean, std) in enumerate(zip(feature_means, feature_stds)):\n",
    "            logger.info(f\"Feature {i} - Mean: {mean:.4f}, Std: {std:.4f}\")\n",
    "\n",
    "    def _scale_data(self, data):\n",
    "        try:\n",
    "            num_samples, seq_len, feat_dim = data.shape\n",
    "            scaled_data = np.zeros_like(data)\n",
    "\n",
    "            for i in range(feat_dim):\n",
    "                feature_data = data[:, :, i].reshape(-1, 1)\n",
    "\n",
    "                # Fit and transform each feature separately\n",
    "                scaled_feature = self.scalers[i].fit_transform(feature_data)\n",
    "                scaled_data[:, :, i] = scaled_feature.reshape(num_samples, seq_len)\n",
    "\n",
    "            logger.info(\"Data scaling completed successfully\")\n",
    "            return scaled_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during data scaling: {e}\")\n",
    "            return data \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.label, dtype=torch.float32)\n",
    "        else:\n",
    "            # For training, we return the input as both input and target\n",
    "            return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.data[idx], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "2024-08-04 21:51:39,631 - __main__ - INFO - Parsed 50000 packets into 3125 samples with 12 features each\n",
      "2024-08-04 21:51:39,675 - __main__ - INFO - Feature 0 - Mean: 2655889408.0000, Std: 1071169536.0000\n",
      "2024-08-04 21:51:39,676 - __main__ - INFO - Feature 1 - Mean: 2757160960.0000, Std: 1072995520.0000\n",
      "2024-08-04 21:51:39,677 - __main__ - INFO - Feature 2 - Mean: 549.8768, Std: 896.5219\n",
      "2024-08-04 21:51:39,677 - __main__ - INFO - Feature 3 - Mean: 1.2742, Std: 0.9615\n",
      "2024-08-04 21:51:39,678 - __main__ - INFO - Feature 4 - Mean: 91.3721, Std: 60.2832\n",
      "2024-08-04 21:51:39,679 - __main__ - INFO - Feature 5 - Mean: 8.6800, Std: 5.5182\n",
      "2024-08-04 21:51:39,679 - __main__ - INFO - Feature 6 - Mean: 13521.8740, Std: 21714.3242\n",
      "2024-08-04 21:51:39,680 - __main__ - INFO - Feature 7 - Mean: 15104.0566, Std: 22016.7422\n",
      "2024-08-04 21:51:39,680 - __main__ - INFO - Feature 8 - Mean: 273.1571, Std: 1157.0841\n",
      "2024-08-04 21:51:39,681 - __main__ - INFO - Feature 9 - Mean: 2491.9138, Std: 9977.2822\n",
      "2024-08-04 21:51:39,681 - __main__ - INFO - Feature 10 - Mean: 563.6039, Std: 896.5561\n",
      "2024-08-04 21:51:39,683 - __main__ - INFO - Feature 11 - Mean: 1379589632.0000, Std: 404926080.0000\n",
      "2024-08-04 21:51:39,683 - __main__ - INFO - Feature 12 - Mean: -1918827.7500, Std: 500248256.0000\n",
      "2024-08-04 21:51:39,683 - __main__ - INFO - Feature 13 - Mean: -127921.7422, Std: 33349980.0000\n",
      "2024-08-04 21:51:39,696 - __main__ - ERROR - Error during data scaling: list index out of range\n",
      "2024-08-04 21:51:52,370 - __main__ - INFO - Parsed 50000 packets into 3125 samples with 12 features each\n",
      "2024-08-04 21:51:52,415 - __main__ - INFO - Feature 0 - Mean: 2655889408.0000, Std: 1071169536.0000\n",
      "2024-08-04 21:51:52,416 - __main__ - INFO - Feature 1 - Mean: 2757160960.0000, Std: 1072995520.0000\n",
      "2024-08-04 21:51:52,417 - __main__ - INFO - Feature 2 - Mean: 549.8768, Std: 896.5219\n",
      "2024-08-04 21:51:52,417 - __main__ - INFO - Feature 3 - Mean: 1.2742, Std: 0.9615\n",
      "2024-08-04 21:51:52,418 - __main__ - INFO - Feature 4 - Mean: 91.3721, Std: 60.2832\n",
      "2024-08-04 21:51:52,418 - __main__ - INFO - Feature 5 - Mean: 8.6800, Std: 5.5182\n",
      "2024-08-04 21:51:52,419 - __main__ - INFO - Feature 6 - Mean: 13521.8740, Std: 21714.3242\n",
      "2024-08-04 21:51:52,419 - __main__ - INFO - Feature 7 - Mean: 15104.0566, Std: 22016.7422\n",
      "2024-08-04 21:51:52,420 - __main__ - INFO - Feature 8 - Mean: 273.1571, Std: 1157.0841\n",
      "2024-08-04 21:51:52,420 - __main__ - INFO - Feature 9 - Mean: 2491.9138, Std: 9977.2822\n",
      "2024-08-04 21:51:52,421 - __main__ - INFO - Feature 10 - Mean: 563.6039, Std: 896.5561\n",
      "2024-08-04 21:51:52,421 - __main__ - INFO - Feature 11 - Mean: 1379589632.0000, Std: 404926080.0000\n",
      "2024-08-04 21:51:52,422 - __main__ - INFO - Feature 12 - Mean: -1918827.7500, Std: 500248256.0000\n",
      "2024-08-04 21:51:52,422 - __main__ - INFO - Feature 13 - Mean: -127921.7422, Std: 33349980.0000\n",
      "2024-08-04 21:51:52,434 - __main__ - ERROR - Error during data scaling: list index out of range\n",
      "2024-08-04 21:52:02,200 - __main__ - INFO - Parsed 50000 packets into 3125 samples with 12 features each\n",
      "2024-08-04 21:52:02,248 - __main__ - INFO - Feature 0 - Mean: 2642009344.0000, Std: 1122806016.0000\n",
      "2024-08-04 21:52:02,249 - __main__ - INFO - Feature 1 - Mean: 2658297088.0000, Std: 1172390656.0000\n",
      "2024-08-04 21:52:02,250 - __main__ - INFO - Feature 2 - Mean: 164.3455, Std: 339.4733\n",
      "2024-08-04 21:52:02,251 - __main__ - INFO - Feature 3 - Mean: 1.7730, Std: 0.6344\n",
      "2024-08-04 21:52:02,251 - __main__ - INFO - Feature 4 - Mean: 82.9456, Std: 43.8031\n",
      "2024-08-04 21:52:02,252 - __main__ - INFO - Feature 5 - Mean: 6.4817, Std: 3.6872\n",
      "2024-08-04 21:52:02,252 - __main__ - INFO - Feature 6 - Mean: 20905.1016, Std: 25548.1094\n",
      "2024-08-04 21:52:02,253 - __main__ - INFO - Feature 7 - Mean: 29468.2012, Std: 27656.5664\n",
      "2024-08-04 21:52:02,254 - __main__ - INFO - Feature 8 - Mean: 42.0120, Std: 231.3598\n",
      "2024-08-04 21:52:02,255 - __main__ - INFO - Feature 9 - Mean: 2594.7119, Std: 9232.7432\n",
      "2024-08-04 21:52:02,255 - __main__ - INFO - Feature 10 - Mean: 178.5970, Std: 339.5289\n",
      "2024-08-04 21:52:02,256 - __main__ - INFO - Feature 11 - Mean: 1573278592.0000, Std: 474709728.0000\n",
      "2024-08-04 21:52:02,256 - __main__ - INFO - Feature 12 - Mean: 23070804.0000, Std: 666521664.0000\n",
      "2024-08-04 21:52:02,256 - __main__ - INFO - Feature 13 - Mean: 1538054.3750, Std: 44447472.0000\n",
      "2024-08-04 21:52:02,268 - __main__ - ERROR - Error during data scaling: list index out of range\n",
      "c:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:5447: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:252.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 1.3772213538859922e+18\n",
      "Epoch 1 loss: 1.3772213538859922e+18\n",
      "Epoch 2 loss: 1.3772213538859922e+18\n",
      "Epoch 3 loss: 1.3772213538859922e+18\n",
      "Epoch 4 loss: 1.3772213538859922e+18\n",
      "Epoch 5 loss: 1.3772213538859922e+18\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 150\u001b[0m\n\u001b[0;32m    147\u001b[0m test_dataloader \u001b[38;5;241m=\u001b[39m create_combined_dataloader(normal_pcap_file, malicious_pcap_file, sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, feature_dim\u001b[38;5;241m=\u001b[39mfeature_dim, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    149\u001b[0m \u001b[38;5;66;03m# Train and test the model\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m trained_model, final_threshold \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m writer \u001b[38;5;241m=\u001b[39m SummaryWriter()\n\u001b[0;32m    153\u001b[0m \u001b[38;5;66;03m# Final evaluation\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 125\u001b[0m, in \u001b[0;36mtrain_and_test\u001b[1;34m(model, train_dataloader, test_dataloader, num_epochs, learning_rate, weight_decay)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Set threshold based on training data\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m epoch \u001b[38;5;241m==\u001b[39m num_epochs \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 125\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[43mset_threshold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     accuracy, precision, recall, f1, recon_errors \u001b[38;5;241m=\u001b[39m test_model(model, test_dataloader, writer, epoch, threshold)\n",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m, in \u001b[0;36mset_threshold\u001b[1;34m(model, train_dataloader, percentile)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, _ \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m     30\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 31\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     recon_error \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_batch_error((outputs, \u001b[38;5;28;01mNone\u001b[39;00m), inputs)\n\u001b[0;32m     33\u001b[0m     reconstruction_errors\u001b[38;5;241m.\u001b[39mextend(recon_error\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\theob\\Code\\Refactored MLSec\\Docker\\model_store\\transformer_autoencoder.py:35\u001b[0m, in \u001b[0;36mTransformerAutoencoder.forward\u001b[1;34m(self, src)\u001b[0m\n\u001b[0;32m     33\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_encoder(src)\n\u001b[0;32m     34\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(src)\n\u001b[1;32m---> 35\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(output)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:460\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    457\u001b[0m tgt_is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 460\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    462\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    464\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    468\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(output)\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:847\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[1;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    846\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[1;32m--> 847\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mha_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    848\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:865\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[1;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_mha_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[0;32m    864\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 865\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultihead_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(x)\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1510\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1519\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1517\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1519\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1228\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1229\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1238\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1239\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:5447\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5444\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m   5445\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m-> 5447\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5448\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[0;32m   5450\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from transformer_autoencoder import TransformerAutoencoder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def create_dataloader(pcap_file, sequence_length, feature_dim, batch_size, label=None):\n",
    "    dataset = PacketSequenceDataset(pcap_file=pcap_file, sequence_length=sequence_length, feature_dim=feature_dim, label=label)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "def create_combined_dataloader(normal_pcap_file, malicious_pcap_file, sequence_length, feature_dim, batch_size):\n",
    "    normal_dataset = PacketSequenceDataset(pcap_file=normal_pcap_file, sequence_length=sequence_length, feature_dim=feature_dim, label=0)\n",
    "    malicious_dataset = PacketSequenceDataset(pcap_file=malicious_pcap_file, sequence_length=sequence_length, feature_dim=feature_dim, label=1)\n",
    "    \n",
    "    combined_data = ConcatDataset([normal_dataset, malicious_dataset])\n",
    "    combined_loader = DataLoader(combined_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return combined_loader\n",
    "\n",
    "def set_threshold(model, train_dataloader, percentile=90):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            recon_error = model.compute_batch_error((outputs, None), inputs)\n",
    "            reconstruction_errors.extend(recon_error.cpu().numpy())\n",
    "    \n",
    "    threshold = np.percentile(reconstruction_errors, percentile)\n",
    "    return threshold\n",
    "\n",
    "def test_model(model, test_dataloader, writer, epoch, threshold):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_recon_errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            recon_errors = model.compute_batch_error((outputs, None), inputs)\n",
    "            \n",
    "            predictions = (recon_errors > threshold).float()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_recon_errors.extend(recon_errors.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_recon_errors = np.array(all_recon_errors)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar('Test/Accuracy', accuracy, epoch)\n",
    "    writer.add_scalar('Test/Precision', precision, epoch)\n",
    "    writer.add_scalar('Test/Recall', recall, epoch)\n",
    "    writer.add_scalar('Test/F1-Score', f1, epoch)\n",
    "\n",
    "    return accuracy, precision, recall, f1, all_recon_errors\n",
    "\n",
    "def print_sample_reconstruction_errors(recon_errors, labels, num_samples=5):\n",
    "    normal_errors = recon_errors[labels == 0]\n",
    "    malicious_errors = recon_errors[labels == 1]\n",
    "\n",
    "    print(\"\\nSample Reconstruction Errors for Normal Data:\")\n",
    "    print(normal_errors[:num_samples])\n",
    "    print(f\"Average Reconstruction Error for Normal Data: {np.mean(normal_errors):.4f}\")\n",
    "\n",
    "    print(\"\\nSample Reconstruction Errors for Malicious Data:\")\n",
    "    print(malicious_errors[:num_samples])\n",
    "    print(f\"Average Reconstruction Error for Malicious Data: {np.mean(malicious_errors):.4f}\")\n",
    "\n",
    "\n",
    "# Main training and testing loop\n",
    "def train_and_test(model, train_dataloader, test_dataloader, num_epochs, learning_rate, weight_decay):\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, _) in enumerate(train_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Data augmentation\n",
    "            inputs_noisy = inputs + 0.1 * torch.randn_like(inputs)\n",
    "            \n",
    "            outputs, _ = model(inputs_noisy)\n",
    "            loss = model.compute_loss((outputs, None), inputs)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch} loss: {avg_epoch_loss}\")\n",
    "        writer.add_scalar('Train/Loss', avg_epoch_loss, epoch)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_epoch_loss)\n",
    "        \n",
    "        # Set threshold based on training data\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            threshold = set_threshold(model, train_dataloader)\n",
    "            \n",
    "            # Test the model\n",
    "            accuracy, precision, recall, f1, recon_errors = test_model(model, test_dataloader, writer, epoch, threshold)\n",
    "    print_sample_reconstruction_errors(recon_errors, np.array([label for _, label in test_dataloader.dataset]), num_samples=5)\n",
    "    writer.close()\n",
    "    return model, threshold\n",
    "\n",
    "# Usage example\n",
    "sequence_length = 16\n",
    "feature_dim = 12\n",
    "num_epochs = 30\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Initialize model and move it to GPU\n",
    "model = TransformerAutoencoder(input_size=12, sequence_length=sequence_length).to(device)\n",
    "\n",
    "normal_pcap_file = 'C:/capture/Monday-WorkingHours.pcap'\n",
    "malicious_pcap_file = 'C:/capture/xenorat_1hr.pcap'\n",
    "\n",
    "train_dataloader = create_dataloader(normal_pcap_file, sequence_length=16, feature_dim=feature_dim, batch_size=4, label=0)\n",
    "test_dataloader = create_combined_dataloader(normal_pcap_file, malicious_pcap_file, sequence_length=16, feature_dim=feature_dim, batch_size=4)\n",
    "\n",
    "# Train and test the model\n",
    "trained_model, final_threshold = train_and_test(model, train_dataloader, test_dataloader, num_epochs, learning_rate, weight_decay)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Final evaluation\n",
    "final_accuracy, final_precision, final_recall, final_f1, final_recon_errors = test_model(trained_model, test_dataloader, writer, num_epochs, final_threshold)\n",
    "\n",
    "print(f\"Final Results - Accuracy: {final_accuracy:.4f}, Precision: {final_precision:.4f}, Recall: {final_recall:.4f}, F1-Score: {final_f1:.4f}\")\n",
    "print(f\"Final Threshold: {final_threshold}\")\n",
    "print(f\"Final Reconstruction Errors: {final_recon_errors}\")\n",
    "print_sample_reconstruction_errors(final_recon_errors, np.array([label for _, label in test_dataloader.dataset]), num_samples=5)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
