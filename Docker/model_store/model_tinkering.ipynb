{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Wireshark is installed, but cannot read manuf !\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from scapy.all import rdpcap, IP, TCP, UDP, PcapReader\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PacketSequenceDataset(Dataset):\n",
    "    def __init__(self, pcap_file, sequence_length, feature_dim, max_packets=50000, label=None):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.feature_dim = feature_dim\n",
    "        self.label = label\n",
    "        self.scalers = [StandardScaler() for _ in range(feature_dim)]\n",
    "        self.max_packets = max_packets\n",
    "        self.data = self._parse_pcap(pcap_file)\n",
    "        self._compute_feature_averages(self.data)\n",
    "        self.data = self._scale_data(self.data)\n",
    "\n",
    "    def _parse_pcap(self, pcap_file):\n",
    "        try:\n",
    "            with PcapReader(pcap_file) as pcap_reader:\n",
    "                packets = [pcap_reader.read_packet() for _ in range(self.max_packets)]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to read pcap file: {e}\")\n",
    "            return np.zeros((1, self.sequence_length, self.feature_dim), dtype=np.float32)\n",
    "        data = []\n",
    "\n",
    "        def safe_convert(value):\n",
    "            if isinstance(value, (int, float)):\n",
    "                return value\n",
    "            try:\n",
    "                return int(value)\n",
    "            except:\n",
    "                try:\n",
    "                    return float(value)\n",
    "                except:\n",
    "                    return 0\n",
    "\n",
    "        expected_feature_count = self.feature_dim  # Use the initial feature_dim\n",
    "\n",
    "        for packet in packets:\n",
    "            features = []\n",
    "            # IP features\n",
    "            try:\n",
    "                if IP in packet:\n",
    "                    ip = packet[IP]\n",
    "                    features.extend([\n",
    "                        int.from_bytes(bytes(map(int, ip.src.split('.'))), byteorder='big'),\n",
    "                        int.from_bytes(bytes(map(int, ip.dst.split('.'))), byteorder='big'),\n",
    "                        safe_convert(ip.len),\n",
    "                        safe_convert(ip.flags),\n",
    "                        #safe_convert(ip.frag),\n",
    "                        safe_convert(ip.ttl),\n",
    "                        safe_convert(ip.proto)\n",
    "                    ])\n",
    "                else:\n",
    "                    features.extend([0] * 7)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error parsing IP features: {e}\")\n",
    "                features.extend([0] * 7)\n",
    "\n",
    "            # TCP/UDP features\n",
    "            try:\n",
    "                if TCP in packet:\n",
    "                    tcp = packet[TCP]\n",
    "                    features.extend([\n",
    "                        safe_convert(tcp.sport),\n",
    "                        safe_convert(tcp.dport),\n",
    "                        safe_convert(tcp.flags),\n",
    "                        safe_convert(tcp.window)\n",
    "                    ])\n",
    "                elif UDP in packet:\n",
    "                    udp = packet[UDP]\n",
    "                    features.extend([\n",
    "                        safe_convert(udp.sport),\n",
    "                        safe_convert(udp.dport),\n",
    "                        #safe_convert(udp.len)\n",
    "                    ])\n",
    "                    features.append(0)  # Padding to match TCP feature count\n",
    "                else:\n",
    "                    features.extend([0] * 4)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error parsing TCP/UDP features: {e}\")\n",
    "                features.extend([0] * 4)\n",
    "\n",
    "            # General packet features\n",
    "            try:\n",
    "                features.extend([\n",
    "                    safe_convert(len(packet))\n",
    "                ])\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error parsing general packet features: {e}\")\n",
    "                features.extend([0, 0.0])\n",
    "\n",
    "            if len(features) < expected_feature_count:\n",
    "                features.extend([0] * (expected_feature_count - len(features)))\n",
    "            elif len(features) > expected_feature_count:\n",
    "                features = features[:expected_feature_count]    \n",
    "\n",
    "            data.append(features)\n",
    "\n",
    "        num_samples = max(1, len(data) // self.sequence_length)\n",
    "        padded_data = np.zeros((num_samples, self.sequence_length, self.feature_dim), dtype=np.float32)\n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            start_idx = i * self.sequence_length\n",
    "            end_idx = min(start_idx + self.sequence_length, len(data))\n",
    "            padded_data[i, :end_idx - start_idx, :] = data[start_idx:end_idx]\n",
    "\n",
    "        logger.info(f\"Parsed {len(data)} packets into {num_samples} samples with {self.feature_dim} features each\")\n",
    "        return np.array(padded_data, dtype=np.float32)\n",
    "    \n",
    "    def _compute_feature_averages(self, data):\n",
    "        feature_means = np.mean(data, axis=(0, 1))\n",
    "        feature_stds = np.std(data, axis=(0, 1))\n",
    "        for i, (mean, std) in enumerate(zip(feature_means, feature_stds)):\n",
    "            logger.info(f\"Feature {i} - Mean: {mean:.4f}, Std: {std:.4f}\")\n",
    "\n",
    "    def _scale_data(self, data):\n",
    "        try:\n",
    "            num_samples, seq_len, feat_dim = data.shape\n",
    "            scaled_data = np.zeros_like(data)\n",
    "\n",
    "            for i in range(feat_dim):\n",
    "                feature_data = data[:, :, i].reshape(-1, 1)\n",
    "\n",
    "                # Fit and transform each feature separately\n",
    "                scaled_feature = self.scalers[i].fit_transform(feature_data)\n",
    "                scaled_data[:, :, i] = scaled_feature.reshape(num_samples, seq_len)\n",
    "\n",
    "            logger.info(\"Data scaling completed successfully\")\n",
    "            return scaled_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during data scaling: {e}\")\n",
    "            return data \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.label, dtype=torch.float32)\n",
    "        else:\n",
    "            # For training, we return the input as both input and target\n",
    "            return torch.tensor(self.data[idx], dtype=torch.float32), torch.tensor(self.data[idx], dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n",
      "2024-08-07 23:06:27,133 - __main__ - INFO - Parsed 50000 packets into 3125 samples with 12 features each\n",
      "2024-08-07 23:06:27,141 - __main__ - INFO - Feature 0 - Mean: 2655889408.0000, Std: 1071169536.0000\n",
      "2024-08-07 23:06:27,142 - __main__ - INFO - Feature 1 - Mean: 2757160960.0000, Std: 1072995520.0000\n",
      "2024-08-07 23:06:27,142 - __main__ - INFO - Feature 2 - Mean: 549.8768, Std: 896.5219\n",
      "2024-08-07 23:06:27,143 - __main__ - INFO - Feature 3 - Mean: 1.2742, Std: 0.9615\n",
      "2024-08-07 23:06:27,143 - __main__ - INFO - Feature 4 - Mean: 91.3721, Std: 60.2832\n",
      "2024-08-07 23:06:27,144 - __main__ - INFO - Feature 5 - Mean: 8.6800, Std: 5.5182\n",
      "2024-08-07 23:06:27,145 - __main__ - INFO - Feature 6 - Mean: 13521.8740, Std: 21714.3242\n",
      "2024-08-07 23:06:27,146 - __main__ - INFO - Feature 7 - Mean: 15104.0566, Std: 22016.7422\n",
      "2024-08-07 23:06:27,146 - __main__ - INFO - Feature 8 - Mean: 245.0459, Std: 1160.1272\n",
      "2024-08-07 23:06:27,148 - __main__ - INFO - Feature 9 - Mean: 2525.9277, Std: 9968.3281\n",
      "2024-08-07 23:06:27,148 - __main__ - INFO - Feature 10 - Mean: 531.8393, Std: 910.4434\n",
      "2024-08-07 23:06:27,148 - __main__ - INFO - Feature 11 - Mean: 4.8454, Std: 36.6381\n",
      "2024-08-07 23:06:27,161 - __main__ - INFO - Data scaling completed successfully\n",
      "2024-08-07 23:06:39,935 - __main__ - INFO - Parsed 50000 packets into 3125 samples with 12 features each\n",
      "2024-08-07 23:06:39,943 - __main__ - INFO - Feature 0 - Mean: 2655889408.0000, Std: 1071169536.0000\n",
      "2024-08-07 23:06:39,944 - __main__ - INFO - Feature 1 - Mean: 2757160960.0000, Std: 1072995520.0000\n",
      "2024-08-07 23:06:39,945 - __main__ - INFO - Feature 2 - Mean: 549.8768, Std: 896.5219\n",
      "2024-08-07 23:06:39,945 - __main__ - INFO - Feature 3 - Mean: 1.2742, Std: 0.9615\n",
      "2024-08-07 23:06:39,946 - __main__ - INFO - Feature 4 - Mean: 91.3721, Std: 60.2832\n",
      "2024-08-07 23:06:39,946 - __main__ - INFO - Feature 5 - Mean: 8.6800, Std: 5.5182\n",
      "2024-08-07 23:06:39,948 - __main__ - INFO - Feature 6 - Mean: 13521.8740, Std: 21714.3242\n",
      "2024-08-07 23:06:39,949 - __main__ - INFO - Feature 7 - Mean: 15104.0566, Std: 22016.7422\n",
      "2024-08-07 23:06:39,950 - __main__ - INFO - Feature 8 - Mean: 245.0459, Std: 1160.1272\n",
      "2024-08-07 23:06:39,950 - __main__ - INFO - Feature 9 - Mean: 2525.9277, Std: 9968.3281\n",
      "2024-08-07 23:06:39,951 - __main__ - INFO - Feature 10 - Mean: 531.8393, Std: 910.4434\n",
      "2024-08-07 23:06:39,951 - __main__ - INFO - Feature 11 - Mean: 4.8454, Std: 36.6381\n",
      "2024-08-07 23:06:39,962 - __main__ - INFO - Data scaling completed successfully\n",
      "2024-08-07 23:06:49,575 - __main__ - INFO - Parsed 50000 packets into 3125 samples with 12 features each\n",
      "2024-08-07 23:06:49,582 - __main__ - INFO - Feature 0 - Mean: 2642009344.0000, Std: 1122806016.0000\n",
      "2024-08-07 23:06:49,583 - __main__ - INFO - Feature 1 - Mean: 2658297088.0000, Std: 1172390656.0000\n",
      "2024-08-07 23:06:49,583 - __main__ - INFO - Feature 2 - Mean: 164.3455, Std: 339.4733\n",
      "2024-08-07 23:06:49,585 - __main__ - INFO - Feature 3 - Mean: 1.7730, Std: 0.6344\n",
      "2024-08-07 23:06:49,585 - __main__ - INFO - Feature 4 - Mean: 82.9456, Std: 43.8031\n",
      "2024-08-07 23:06:49,586 - __main__ - INFO - Feature 5 - Mean: 6.4817, Std: 3.6872\n",
      "2024-08-07 23:06:49,586 - __main__ - INFO - Feature 6 - Mean: 20905.1016, Std: 25548.1094\n",
      "2024-08-07 23:06:49,587 - __main__ - INFO - Feature 7 - Mean: 29468.2012, Std: 27656.5664\n",
      "2024-08-07 23:06:49,587 - __main__ - INFO - Feature 8 - Mean: 26.2433, Std: 223.8314\n",
      "2024-08-07 23:06:49,588 - __main__ - INFO - Feature 9 - Mean: 2613.3408, Std: 9227.9424\n",
      "2024-08-07 23:06:49,588 - __main__ - INFO - Feature 10 - Mean: 160.0297, Std: 340.5230\n",
      "2024-08-07 23:06:49,589 - __main__ - INFO - Feature 11 - Mean: 4.8817, Std: 16.3869\n",
      "2024-08-07 23:06:49,600 - __main__ - INFO - Data scaling completed successfully\n",
      "c:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\theob\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:5447: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:252.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss: 0.49031896564379696\n",
      "Epoch 1 loss: 0.26898215786123436\n",
      "Epoch 2 loss: 0.1933416295319643\n",
      "Epoch 3 loss: 0.1424606397018199\n",
      "Epoch 4 loss: 0.11391691678643666\n",
      "\n",
      "Sample Reconstruction Errors for Normal Data:\n",
      "[0.10546601 0.11161682 0.07501477 0.24238718 0.1563133 ]\n",
      "Average Reconstruction Error for Normal Data: 0.1728\n",
      "\n",
      "Sample Reconstruction Errors for Malicious Data:\n",
      "[0.061283   0.10091195 0.5064202  0.12980637 0.07665806]\n",
      "Average Reconstruction Error for Malicious Data: 0.1653\n",
      "Final Results - Accuracy: 0.7485, Precision: 0.8564, Recall: 0.5971, F1-Score: 0.7036\n",
      "Final Threshold: 0.14249737858772285\n",
      "Final Reconstruction Errors: [0.03599071 0.26369673 0.11410473 ... 0.07254937 0.33192325 0.1412406 ]\n",
      "\n",
      "Sample Reconstruction Errors for Normal Data:\n",
      "[0.03599071 0.26369673 0.11410473 0.18242094 0.11190029]\n",
      "Average Reconstruction Error for Normal Data: 0.1691\n",
      "\n",
      "Sample Reconstruction Errors for Malicious Data:\n",
      "[0.06639436 0.0926468  0.17645249 0.13074188 0.15478602]\n",
      "Average Reconstruction Error for Malicious Data: 0.1690\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from transformer_autoencoder import TransformerAutoencoder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def create_dataloader(pcap_file, sequence_length, feature_dim, batch_size, label=None):\n",
    "    dataset = PacketSequenceDataset(pcap_file=pcap_file, sequence_length=sequence_length, feature_dim=feature_dim, label=label)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "def create_combined_dataloader(normal_pcap_file, malicious_pcap_file, sequence_length, feature_dim, batch_size):\n",
    "    normal_dataset = PacketSequenceDataset(pcap_file=normal_pcap_file, sequence_length=sequence_length, feature_dim=feature_dim, label=0)\n",
    "    malicious_dataset = PacketSequenceDataset(pcap_file=malicious_pcap_file, sequence_length=sequence_length, feature_dim=feature_dim, label=1)\n",
    "    \n",
    "    combined_data = ConcatDataset([normal_dataset, malicious_dataset])\n",
    "    combined_loader = DataLoader(combined_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return combined_loader\n",
    "\n",
    "def set_threshold(model, train_dataloader, percentile=90):\n",
    "    model.eval()\n",
    "    reconstruction_errors = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in train_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            recon_error = model.compute_batch_error((outputs, None), inputs)\n",
    "            reconstruction_errors.extend(recon_error.cpu().numpy())\n",
    "    \n",
    "    threshold = np.percentile(reconstruction_errors, percentile)\n",
    "    return threshold\n",
    "\n",
    "def test_model(model, test_dataloader, writer, epoch, threshold):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_recon_errors = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs, _ = model(inputs)\n",
    "            recon_errors = model.compute_batch_error((outputs, None), inputs)\n",
    "            \n",
    "            predictions = (recon_errors > threshold).float()\n",
    "            \n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_recon_errors.extend(recon_errors.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_recon_errors = np.array(all_recon_errors)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar('Test/Accuracy', accuracy, epoch)\n",
    "    writer.add_scalar('Test/Precision', precision, epoch)\n",
    "    writer.add_scalar('Test/Recall', recall, epoch)\n",
    "    writer.add_scalar('Test/F1-Score', f1, epoch)\n",
    "\n",
    "    return accuracy, precision, recall, f1, all_recon_errors\n",
    "\n",
    "def print_sample_reconstruction_errors(recon_errors, labels, num_samples=5):\n",
    "    normal_errors = recon_errors[labels == 0]\n",
    "    malicious_errors = recon_errors[labels == 1]\n",
    "\n",
    "    print(\"\\nSample Reconstruction Errors for Normal Data:\")\n",
    "    print(normal_errors[:num_samples])\n",
    "    print(f\"Average Reconstruction Error for Normal Data: {np.mean(normal_errors):.4f}\")\n",
    "\n",
    "    print(\"\\nSample Reconstruction Errors for Malicious Data:\")\n",
    "    print(malicious_errors[:num_samples])\n",
    "    print(f\"Average Reconstruction Error for Malicious Data: {np.mean(malicious_errors):.4f}\")\n",
    "\n",
    "\n",
    "# Main training and testing loop\n",
    "def train_and_test(model, train_dataloader, test_dataloader, num_epochs, learning_rate, weight_decay):\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for batch_idx, (inputs, _) in enumerate(train_dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Data augmentation\n",
    "            inputs_noisy = inputs + 0.1 * torch.randn_like(inputs)\n",
    "            \n",
    "            outputs, _ = model(inputs_noisy)\n",
    "            loss = model.compute_loss((outputs, None), inputs)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch} loss: {avg_epoch_loss}\")\n",
    "        writer.add_scalar('Train/Loss', avg_epoch_loss, epoch)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_epoch_loss)\n",
    "        \n",
    "        # Set threshold based on training data\n",
    "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
    "            threshold = set_threshold(model, train_dataloader)\n",
    "            \n",
    "            # Test the model\n",
    "            accuracy, precision, recall, f1, recon_errors = test_model(model, test_dataloader, writer, epoch, threshold)\n",
    "    print_sample_reconstruction_errors(recon_errors, np.array([label for _, label in test_dataloader.dataset]), num_samples=5)\n",
    "    writer.close()\n",
    "    return model, threshold\n",
    "\n",
    "# Usage example\n",
    "sequence_length = 16\n",
    "feature_dim = 12\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "# Initialize model and move it to GPU\n",
    "model = TransformerAutoencoder(input_size=12, sequence_length=sequence_length).to(device)\n",
    "\n",
    "normal_pcap_file = 'C:/capture/Monday-WorkingHours.pcap'\n",
    "malicious_pcap_file = 'C:/capture/xenorat_1hr.pcap'\n",
    "\n",
    "train_dataloader = create_dataloader(normal_pcap_file, sequence_length=16, feature_dim=feature_dim, batch_size=4, label=0)\n",
    "test_dataloader = create_combined_dataloader(normal_pcap_file, malicious_pcap_file, sequence_length=16, feature_dim=feature_dim, batch_size=4)\n",
    "\n",
    "# Train and test the model\n",
    "trained_model, final_threshold = train_and_test(model, train_dataloader, test_dataloader, num_epochs, learning_rate, weight_decay)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Final evaluation\n",
    "final_accuracy, final_precision, final_recall, final_f1, final_recon_errors = test_model(trained_model, test_dataloader, writer, num_epochs, final_threshold)\n",
    "\n",
    "print(f\"Final Results - Accuracy: {final_accuracy:.4f}, Precision: {final_precision:.4f}, Recall: {final_recall:.4f}, F1-Score: {final_f1:.4f}\")\n",
    "print(f\"Final Threshold: {final_threshold}\")\n",
    "print(f\"Final Reconstruction Errors: {final_recon_errors}\")\n",
    "print_sample_reconstruction_errors(final_recon_errors, np.array([label for _, label in test_dataloader.dataset]), num_samples=5)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "model_file_name = f'transformer_autoencoder_base.pth'\n",
    "model_save_path = os.path.join('./', model_file_name)\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
