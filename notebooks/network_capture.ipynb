{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'scripts')))\n",
    "\n",
    "# Define the path to the config.yaml file\n",
    "config_path = os.path.abspath(os.path.join('..', 'configs', 'config.yaml'))\n",
    "\n",
    "# Load the YAML file\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Wireshark is installed, but cannot read manuf !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theob\\Code\\Refactored MLSec\\data\\raw\\data.txt\n"
     ]
    }
   ],
   "source": [
    "from capture_data import NetworkTrafficCollector\n",
    "\n",
    "raw_path = config['data']['raw_path']\n",
    "print(raw_path)\n",
    "collection = NetworkTrafficCollector(collection_point=\"Wi-Fi 2\", file_path=raw_path)\n",
    "#collection.start_capture(timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\theob\\Code\\Refactored MLSec\\data\\new_raw\\new_data.txt\n"
     ]
    }
   ],
   "source": [
    "from capture_data import NetworkTrafficCollector\n",
    "\n",
    "new_raw_path = config['data']['new_raw_path']\n",
    "print(new_raw_path)\n",
    "new_collection = NetworkTrafficCollector(collection_point=\"Wi-Fi 2\", file_path=new_raw_path)\n",
    "#new_collection.start_capture(timeout=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "INFO:root:Reading data from C:\\Users\\theob\\Code\\Refactored MLSec\\data\\raw\\data.txt\n",
      "INFO:root:Parsed 2236 packets\n",
      "INFO:root:Scaled data written to C:\\Users\\theob\\Code\\Refactored MLSec\\data\\processed\\processed_data.h5\n",
      "INFO:root:Scaler saved to C:/Users/theob/Code/Refactored MLSec/Docker/traffic_capture/model/minmaxscaler.pkl\n"
     ]
    }
   ],
   "source": [
    "from preprocess_data import process_packets\n",
    "\n",
    "raw_path = config['data']['raw_path']\n",
    "processed_path = config['data']['processed_path']\n",
    "scaler_path = 'C:/Users/theob/Code/Refactored MLSec/Docker/traffic_capture/model/minmaxscaler.pkl'\n",
    "process_packets(raw_path, processed_path, scaler_path)\n",
    "\n",
    "new_raw_path = config['data']['new_raw_path']\n",
    "new_processed_path = config['data']['new_processed_path']\n",
    "#process_packets(new_raw_path, new_processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the autoencoder\n",
    "from autoencoder import evaluate_autoencoder, store_autoencoder, train_autoencoder\n",
    "\n",
    "autoencoder = train_autoencoder(new_processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the autoencoder\n",
    "evaluate_autoencoder(autoencoder, new_processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Store the trained autoencoder\n",
    "model_path = config['model']['trained_model_path']\n",
    "store_autoencoder(autoencoder, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "from autoencoder import Autoencoder\n",
    "from torch import nn\n",
    "\n",
    "def load_model(model_path, input_dim):\n",
    "    model = Autoencoder(input_dim)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Function to predict anomalies in new packets\n",
    "def predict_anomalies(model, new_packets, threshold=0.1):\n",
    "    new_packets = torch.tensor(new_packets, dtype=torch.float32)\n",
    "    outputs = model(new_packets)\n",
    "    loss = nn.functional.mse_loss(outputs, new_packets, reduction='none')\n",
    "    loss = loss.mean(dim=1)\n",
    "    anomalies = loss > threshold\n",
    "    print(loss)\n",
    "    return anomalies.numpy()\n",
    "\n",
    "def parse_packet(packet):\n",
    "        match = re.match(r'Ether / IP / (TCP|UDP) ([\\d.]+):(\\w+) > ([\\d.]+):(\\w+) (\\w+)', packet.strip())\n",
    "        if match:\n",
    "            protocol, src_ip, src_port, dst_ip, dst_port, flags = match.groups()\n",
    "            return src_ip, src_port, dst_ip, dst_port, protocol, flags\n",
    "        return None\n",
    "\n",
    "def preprocess_new_packets(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        raw_packets = file.readlines()\n",
    "\n",
    "    packets = [parse_packet(packet) for packet in raw_packets if parse_packet(packet)]\n",
    "    df = pd.DataFrame(packets, columns=['src_ip', 'src_port', 'dst_ip', 'dst_port', 'protocol', 'flags'])\n",
    "\n",
    "    def ip_to_numeric(ip):\n",
    "        return int(ip.replace('.', ''))\n",
    "\n",
    "    def port_to_numeric(port):\n",
    "        try:\n",
    "            if port.lower() == 'https':\n",
    "                return 443\n",
    "            return int(port, 16) if '0x' in port else int(port)\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "    df['src_ip'] = df['src_ip'].apply(ip_to_numeric)\n",
    "    df['dst_ip'] = df['dst_ip'].apply(ip_to_numeric)\n",
    "    df['protocol'] = df['protocol'].astype('category').cat.codes\n",
    "    df['flags'] = df['flags'].astype('category').cat.codes\n",
    "    df['src_port'] = df['src_port'].apply(port_to_numeric)\n",
    "    df['dst_port'] = df['dst_port'].apply(port_to_numeric)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_packets = scaler.fit_transform(df)\n",
    "\n",
    "    return scaled_packets, len(packets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_path = config['model']['trained_model_path']\n",
    "\n",
    "new_packets, num_valid_packets = preprocess_new_packets(new_raw_path)\n",
    "\n",
    "# Load the trained model\n",
    "input_dim = new_packets.shape[1]\n",
    "model = load_model(trained_model_path, input_dim)\n",
    "\n",
    "# Predict anomalies\n",
    "anomalies = predict_anomalies(model, new_packets)\n",
    "\n",
    "# Print results\n",
    "with open(new_raw_path, 'r') as file:\n",
    "    new_raw_packets = file.readlines()\n",
    "\n",
    "valid_packet_idx = 0\n",
    "for packet in new_raw_packets:\n",
    "    if parse_packet(packet):\n",
    "        is_anomalous = anomalies[valid_packet_idx]\n",
    "        status = \"Anomalous\" if is_anomalous else \"Normal\"\n",
    "        print(f\"Packet: {packet.strip()}, Status: {status}\")\n",
    "        valid_packet_idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
